{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 ‚Äî Gradio UI for FIA IBMS Chatbot\n",
    "**Interface:** Dark-themed chat UI with smart routing, token-by-token streaming.\n",
    "\n",
    "**Routing:** Database queries ‚Üí NL2SQL pipeline | General questions ‚Üí qwen3 direct\n",
    "\n",
    "**Qwen3 parameters:** Official recommended settings for non-thinking mode.\n",
    "\n",
    "**Gradio:** 6.4.0 | **Port:** 7861"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradio version: 6.4.0\n",
      "SQL model:       qwen2.5-coder:14b\n",
      "Chat/Narration:  qwen3-14b-fixed\n",
      "Qwen3 options:   {'temperature': 0.7, 'top_p': 0.8, 'top_k': 20, 'min_p': 0, 'repeat_penalty': 1.5, 'num_predict': 2048}\n"
     ]
    }
   ],
   "source": [
    "import oracledb\n",
    "from sqlalchemy import create_engine, text\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import ollama\n",
    "import gradio as gr\n",
    "from pathlib import Path\n",
    "\n",
    "# === Paths ===\n",
    "PROJECT_DIR = Path.home() / \"ml-projects\" / \"python-projects\" / \"IBMS_LLM\"\n",
    "CONFIG_DIR  = PROJECT_DIR / \"Config\"\n",
    "\n",
    "# === Models ===\n",
    "SQL_MODEL       = \"qwen2.5-coder:14b\"\n",
    "NARRATION_MODEL = \"qwen3-14b-fixed\"\n",
    "CHAT_MODEL      = \"qwen3-14b-fixed\"\n",
    "\n",
    "# === Official Qwen3 Non-Thinking Mode Parameters ===\n",
    "# Source: https://ollama.com/dengcao/Qwen3-14B\n",
    "QWEN3_OPTIONS = {\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.8,\n",
    "    \"top_k\": 20,\n",
    "    \"min_p\": 0,\n",
    "    \"repeat_penalty\": 1.5,   # Critical for quantized models ‚Äî stops repetition\n",
    "    \"num_predict\": 2048,\n",
    "}\n",
    "\n",
    "print(f\"Gradio version: {gr.__version__}\")\n",
    "print(f\"SQL model:       {SQL_MODEL}\")\n",
    "print(f\"Chat/Narration:  {CHAT_MODEL}\")\n",
    "print(f\"Qwen3 options:   {QWEN3_OPTIONS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Oracle Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oracle connection OK: (1,)\n"
     ]
    }
   ],
   "source": [
    "engine = create_engine(\n",
    "    \"oracle+oracledb://ibms_user:ibms_pass@localhost:1521/?service_name=FREEPDB1\",\n",
    "    pool_pre_ping=True,\n",
    "    pool_size=3,\n",
    ")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT 1 FROM dual\"))\n",
    "    print(\"Oracle connection OK:\", result.fetchone())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Load SQL Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQL prompt loaded: 13,474 chars\n"
     ]
    }
   ],
   "source": [
    "PROMPT_TEMPLATE = (CONFIG_DIR / \"prompt_template.txt\").read_text()\n",
    "print(f\"SQL prompt loaded: {len(PROMPT_TEMPLATE):,} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Query Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [DATABASE] How many travelers?\n",
      "  [ GENERAL] Hello\n",
      "  [ GENERAL] Why is that so high?\n",
      "  [DATABASE] Show top 10\n"
     ]
    }
   ],
   "source": [
    "CLASSIFIER_PROMPT = \"\"\"Classify this message as DATABASE or GENERAL.\n",
    "\n",
    "DATABASE = needs data from IBMS database (counts, lists, lookups, comparisons, statistics)\n",
    "GENERAL = greeting, follow-up, explanation, opinion, or anything NOT needing a new database query\n",
    "\n",
    "Reply with one word only: DATABASE or GENERAL\\n/no_think\n",
    "\n",
    "Message: {message}\"\"\"\n",
    "\n",
    "\n",
    "def classify_query(message: str, history: list[dict]) -> str:\n",
    "    prompt = CLASSIFIER_PROMPT.replace(\"{message}\", message)\n",
    "    if history:\n",
    "        recent = history[-4:]\n",
    "        context = \"\\n\".join(f\"{m.get('role','')}: {m.get('content','')[:150]}\" for m in recent)\n",
    "        prompt += f\"\\n\\nContext:\\n{context}\"\n",
    "    \n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=CHAT_MODEL,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            options={\"temperature\": 0.0, \"num_predict\": 10, \"repeat_penalty\": 1.5},\n",
    "        )\n",
    "        result = response[\"message\"][\"content\"].strip().upper()\n",
    "        return \"DATABASE\" if \"DATABASE\" in result else \"GENERAL\"\n",
    "    except Exception:\n",
    "        return \"DATABASE\"\n",
    "\n",
    "\n",
    "# Quick test\n",
    "for msg in [\"How many travelers?\", \"Hello\", \"Why is that so high?\", \"Show top 10\"]:\n",
    "    print(f\"  [{classify_query(msg, []):>8}] {msg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Prompts ‚Äî General Chat & Narration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompts defined.\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT_GENERAL = \"\"\"You are an AI assistant for FIA (Federal Investigation Agency) Pakistan, specializing in the IBMS (Integrated Border Management System).\n",
    "\n",
    "Guidelines:\n",
    "- Match your response length to the question. Short questions get short answers. Detailed questions get detailed answers.\n",
    "- For greetings (hello, hi, etc.), respond briefly and warmly. Introduce yourself in 1-2 sentences and ask how you can help.\n",
    "- For follow-up questions about previous answers, provide relevant analysis or clarification.\n",
    "- For FIA/IBMS concept questions, explain clearly with relevant context.\n",
    "- If the officer needs specific data, suggest they ask a data question.\n",
    "- Be professional but conversational. Do NOT pad responses with unnecessary information.\n",
    "- Do NOT invent stories or hypothetical scenarios unless explicitly asked.\"\"\"\n",
    "\n",
    "\n",
    "NARRATION_PROMPT = \"\"\"You are a senior FIA intelligence analyst. An officer asked a question and the system queried the IBMS database. Below are the results.\n",
    "\n",
    "Write a professional intelligence briefing based ONLY on the data provided.\n",
    "\n",
    "Rules:\n",
    "- Lead with the key finding that directly answers the question.\n",
    "- Include all important numbers exactly as shown (counts, percentages, dates).\n",
    "- If results have rankings/tables, present and analyze them.\n",
    "- If 0 rows returned, say \"No records found\" and suggest why.\n",
    "- Do NOT invent data. Do NOT mention SQL or databases.\n",
    "- Match response length to complexity: simple counts get 2-3 sentences, complex analyses get detailed paragraphs.\n",
    "- End with a brief operational insight when the data warrants it.\n",
    "- Do NOT pad your response to fill space. Be thorough but not verbose.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "RESULTS:\n",
    "{results}\n",
    "\n",
    "Briefing:\"\"\"\n",
    "\n",
    "\n",
    "print(\"Prompts defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: NL2SQL Pipeline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline functions loaded.\n"
     ]
    }
   ],
   "source": [
    "def extract_sql(raw: str) -> str:\n",
    "    raw = raw.strip()\n",
    "    match = re.search(r'```(?:sql)?\\s*\\n?(.*?)\\n?```', raw, re.DOTALL | re.IGNORECASE)\n",
    "    if match:\n",
    "        sql = match.group(1).strip()\n",
    "    else:\n",
    "        match = re.search(r'(SELECT\\b.*)', raw, re.DOTALL | re.IGNORECASE)\n",
    "        sql = match.group(1).strip() if match else raw\n",
    "    if ';' in sql:\n",
    "        sql = sql[:sql.index(';')].strip()\n",
    "    return sql\n",
    "\n",
    "\n",
    "def generate_sql(question: str) -> tuple[str, str, float]:\n",
    "    prompt = PROMPT_TEMPLATE.replace(\"{question}\", question)\n",
    "    t0 = time.time()\n",
    "    response = ollama.chat(\n",
    "        model=SQL_MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        options={\"temperature\": 0.0, \"num_predict\": 1024},\n",
    "    )\n",
    "    latency = time.time() - t0\n",
    "    raw = response[\"message\"][\"content\"]\n",
    "    return raw, extract_sql(raw), latency\n",
    "\n",
    "\n",
    "IBMS_TABLES = {\n",
    "    \"countries\", \"ports_of_entry\", \"visa_categories\", \"sponsors\",\n",
    "    \"travelers\", \"document_registry\", \"visa_applications\", \"travel_records\",\n",
    "    \"asylum_claims\", \"removal_orders\", \"detention_records\",\n",
    "    \"family_relationships\", \"watchlist\", \"ecl_entries\",\n",
    "    \"trafficking_cases\", \"illegal_crossings\", \"offloading_records\",\n",
    "    \"risk_profiles\", \"suspect_networks\", \"audit_log\",\n",
    "}\n",
    "\n",
    "BLOCKED_KEYWORDS = [\n",
    "    r\"\\bINSERT\\b\", r\"\\bUPDATE\\b\", r\"\\bDELETE\\b\", r\"\\bMERGE\\b\",\n",
    "    r\"\\bCREATE\\b\", r\"\\bDROP\\b\", r\"\\bALTER\\b\", r\"\\bTRUNCATE\\b\", r\"\\bRENAME\\b\",\n",
    "    r\"\\bGRANT\\b\", r\"\\bREVOKE\\b\",\n",
    "    r\"\\bDBMS_\", r\"\\bUTL_\", r\"\\bSYS\\.\", r\"\\bDBA_\",\n",
    "    r\"\\bV\\$\", r\"\\bEXECUTE\\s+IMMEDIATE\\b\",\n",
    "    r\"\\bBEGIN\\b\", r\"\\bDECLARE\\b\", r\"\\bEXEC\\b\",\n",
    "]\n",
    "\n",
    "\n",
    "def validate_sql(sql: str) -> tuple[bool, str]:\n",
    "    if not sql or not sql.strip():\n",
    "        return False, \"Empty SQL\"\n",
    "    sql_upper = sql.strip().upper()\n",
    "    if not (sql_upper.startswith(\"SELECT\") or sql_upper.startswith(\"WITH\")):\n",
    "        return False, f\"Must start with SELECT/WITH. Got: {sql_upper[:30]}\"\n",
    "    if ';' in sql:\n",
    "        return False, \"Multiple statements detected\"\n",
    "    for pattern in BLOCKED_KEYWORDS:\n",
    "        match = re.search(pattern, sql, re.IGNORECASE)\n",
    "        if match:\n",
    "            return False, f\"Blocked: {match.group()}\"\n",
    "    if '--' in sql or '/*' in sql:\n",
    "        return False, \"SQL comments not allowed\"\n",
    "    sql_lower = sql.lower()\n",
    "    if not any(t in sql_lower for t in IBMS_TABLES):\n",
    "        return False, \"No known IBMS table referenced\"\n",
    "    return True, \"OK\"\n",
    "\n",
    "\n",
    "def execute_sql(sql: str) -> tuple[bool, pd.DataFrame | None, str, float]:\n",
    "    is_valid, reason = validate_sql(sql)\n",
    "    if not is_valid:\n",
    "        return False, None, f\"Validation failed: {reason}\", 0.0\n",
    "    try:\n",
    "        t0 = time.time()\n",
    "        with engine.connect() as conn:\n",
    "            df = pd.read_sql(text(sql), conn)\n",
    "        exec_time = time.time() - t0\n",
    "        return True, df, f\"{len(df)} rows in {exec_time:.2f}s\", exec_time\n",
    "    except Exception as e:\n",
    "        return False, None, f\"Execution error: {str(e)[:200]}\", 0.0\n",
    "\n",
    "\n",
    "print(\"Pipeline functions loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Streaming Helper\n",
    "Uses `ollama.chat(stream=True)` with official Qwen3 parameters for token-by-token output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stream_llm_response() defined.\n"
     ]
    }
   ],
   "source": [
    "def clean_qwen3_output(text: str) -> str:\n",
    "    \"\"\"Clean up common qwen3 artifacts.\"\"\"\n",
    "    text = re.sub(r'<think>.*?</think>\\s*', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'<think>(?:(?!</think>).)*$', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'^(A:\\s*\\n?)+', '', text)\n",
    "    text = re.sub(r'(Okay,.*?(done|ready|complete|wrap it up|finalize|all set)[.\\s]*)+$', '', text, flags=re.DOTALL)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def stream_llm_response(model: str, messages: list[dict], options: dict = None):\n",
    "    \"\"\"Generator yielding accumulated cleaned text token-by-token.\n",
    "    \n",
    "    NOTE: Ollama think=False is unreliable for qwen3.\n",
    "    Workaround: System prompt embedded in user message + /no_think appended.\n",
    "    \"\"\"\n",
    "    opts = dict(QWEN3_OPTIONS)\n",
    "    if options:\n",
    "        opts.update(options)\n",
    "    \n",
    "    accumulated = \"\"\n",
    "    \n",
    "    stream = ollama.chat(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        options=opts,\n",
    "        stream=True,\n",
    "    )\n",
    "    \n",
    "    for chunk in stream:\n",
    "        token = chunk.get(\"message\", {}).get(\"content\", \"\")\n",
    "        if token:\n",
    "            accumulated += token\n",
    "            cleaned = clean_qwen3_output(accumulated)\n",
    "            if cleaned:\n",
    "                yield cleaned\n",
    "    \n",
    "    yield clean_qwen3_output(accumulated)\n",
    "\n",
    "\n",
    "print(\"stream_llm_response() defined.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Main Chat Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat handler defined.\n"
     ]
    }
   ],
   "source": [
    "def chat_handler_streaming(message: str, history: list[dict]):\n",
    "    \"\"\"Smart routing + streaming. Ollama workarounds applied.\"\"\"\n",
    "    if not message.strip():\n",
    "        yield \"Please enter a question.\"\n",
    "        return\n",
    "    \n",
    "    yield \"üîç **Analyzing your question...**\"\n",
    "    query_type = classify_query(message, history)\n",
    "    \n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    # GENERAL PATH\n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    if query_type == \"GENERAL\":\n",
    "        context = \"\"\n",
    "        if history:\n",
    "            for msg in history[-10:]:\n",
    "                role = msg.get(\"role\", \"\")\n",
    "                content = msg.get(\"content\", \"\")[:500]\n",
    "                context += f\"{role}: {content}\\n\"\n",
    "        \n",
    "        # Embed system prompt in user message (Ollama ignores system role for qwen3)\n",
    "        combined = SYSTEM_PROMPT_GENERAL + \"\\n\\n\"\n",
    "        if context:\n",
    "            combined += f\"Previous conversation:\\n{context}\\n\\n\"\n",
    "        combined += f\"Officer's message: {message}\\n\\nRespond now. /no_think\"\n",
    "        \n",
    "        messages = [{\"role\": \"user\", \"content\": combined}]\n",
    "        \n",
    "        t0 = time.time()\n",
    "        last_text = \"\"\n",
    "        \n",
    "        for partial_text in stream_llm_response(CHAT_MODEL, messages):\n",
    "            last_text = partial_text\n",
    "            yield partial_text\n",
    "        \n",
    "        latency = time.time() - t0\n",
    "        \n",
    "        footer = f\"\\n\\n---\\n\"\n",
    "        footer += f\"<details>\\n\"\n",
    "        footer += f\"<summary>‚ÑπÔ∏è Response Info</summary>\\n\\n\"\n",
    "        footer += f\"**Mode:** General conversation\\n\\n\"\n",
    "        footer += f\"**Model:** {CHAT_MODEL}\\n\\n\"\n",
    "        footer += f\"**Time:** {latency:.1f}s\\n\"\n",
    "        footer += f\"</details>\"\n",
    "        \n",
    "        yield last_text + footer\n",
    "        return\n",
    "    \n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    # DATABASE PATH\n",
    "    # ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê\n",
    "    pipeline_start = time.time()\n",
    "    \n",
    "    yield \"‚è≥ **Generating SQL...**\"\n",
    "    try:\n",
    "        raw, sql, gen_time = generate_sql(message)\n",
    "    except Exception as e:\n",
    "        yield f\"‚ùå SQL generation failed: {str(e)[:200]}\"\n",
    "        return\n",
    "    \n",
    "    yield f\"‚úÖ SQL generated ({gen_time:.1f}s)\\n‚è≥ **Validating...**\"\n",
    "    is_valid, val_msg = validate_sql(sql)\n",
    "    if not is_valid:\n",
    "        yield f\"‚ö†Ô∏è Query blocked: {val_msg}\\n\\nPlease rephrase your question.\"\n",
    "        return\n",
    "    \n",
    "    yield f\"‚úÖ SQL generated ({gen_time:.1f}s)\\n‚úÖ Validated\\n‚è≥ **Executing on Oracle...**\"\n",
    "    try:\n",
    "        exec_success, df, exec_msg, exec_time = execute_sql(sql)\n",
    "    except Exception as e:\n",
    "        yield f\"‚ùå Database error: {str(e)[:200]}\"\n",
    "        return\n",
    "    if not exec_success:\n",
    "        yield f\"‚ö†Ô∏è Execution failed: {exec_msg}\\n\\nPlease rephrase.\"\n",
    "        return\n",
    "    \n",
    "    row_count = len(df) if df is not None else 0\n",
    "    \n",
    "    yield (\n",
    "        f\"‚úÖ SQL generated ({gen_time:.1f}s)\\n\"\n",
    "        f\"‚úÖ Validated\\n\"\n",
    "        f\"‚úÖ Executed ({row_count} rows, {exec_time:.2f}s)\\n\\n\"\n",
    "        f\"‚è≥ **Narrating results...**\"\n",
    "    )\n",
    "    \n",
    "    if df is None or df.empty:\n",
    "        results_text = \"(No results ‚Äî 0 rows returned)\"\n",
    "    else:\n",
    "        display_df = df.head(50)\n",
    "        results_text = display_df.to_string(index=False)\n",
    "        if len(df) > 50:\n",
    "            results_text += f\"\\n\\n... ({len(df)} total rows, showing first 50)\"\n",
    "    \n",
    "    nar_prompt = NARRATION_PROMPT.replace(\"{question}\", message).replace(\"{results}\", results_text)\n",
    "    nar_prompt += \"\\n/no_think\"\n",
    "    nar_messages = [{\"role\": \"user\", \"content\": nar_prompt}]\n",
    "    \n",
    "    nar_start = time.time()\n",
    "    last_text = \"\"\n",
    "    \n",
    "    for partial_narration in stream_llm_response(NARRATION_MODEL, nar_messages):\n",
    "        last_text = partial_narration\n",
    "        yield partial_narration\n",
    "    \n",
    "    nar_time = time.time() - nar_start\n",
    "    total_time = time.time() - pipeline_start\n",
    "    \n",
    "    footer = f\"\\n\\n---\\n\"\n",
    "    footer += f\"<details>\\n\"\n",
    "    footer += f\"<summary>üìä Query Details (click to expand)</summary>\\n\\n\"\n",
    "    footer += f\"**Mode:** Database query (NL2SQL)\\n\\n\"\n",
    "    footer += f\"**Generated SQL:**\\n```sql\\n{sql}\\n```\\n\\n\"\n",
    "    footer += f\"**Execution:** {row_count} rows in {exec_time:.2f}s\\n\\n\"\n",
    "    footer += f\"**Timings:** SQL Gen: {gen_time:.1f}s ‚îÇ Exec: {exec_time:.2f}s ‚îÇ Narration: {nar_time:.1f}s ‚îÇ **Total: {total_time:.1f}s**\\n\"\n",
    "    footer += f\"</details>\"\n",
    "    \n",
    "    yield last_text + footer\n",
    "\n",
    "\n",
    "print(\"Chat handler defined.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Custom CSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSS defined.\n"
     ]
    }
   ],
   "source": [
    "CUSTOM_CSS = \"\"\"\n",
    ".gradio-container {\n",
    "    max-width: 1000px !important;\n",
    "    margin: auto !important;\n",
    "}\n",
    "details summary {\n",
    "    cursor: pointer;\n",
    "    font-weight: 600;\n",
    "    opacity: 0.8;\n",
    "    font-size: 0.85rem;\n",
    "}\n",
    "details summary:hover {\n",
    "    opacity: 1.0;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "print(\"CSS defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Build & Launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EXAMPLE_QUERIES = [\n",
    "    \"How many travelers are in the system?\",\n",
    "    \"List all off-loaded passengers at Islamabad Airport in 2025\",\n",
    "    \"Which airlines have the highest off-loading rate?\",\n",
    "    \"How many watchlist alerts are currently active?\",\n",
    "    \"Top 10 most frequent travelers this year\",\n",
    "    \"Compare off-loading rates across all airports\",\n",
    "    \"What is IBMS?\",\n",
    "    \"What does offloading mean in FIA context?\",\n",
    "]\n",
    "\n",
    "app = gr.ChatInterface(\n",
    "    fn=chat_handler_streaming,\n",
    "    title=\"üîê FIA IBMS Intelligence Assistant\",\n",
    "    description=\"Ask data questions (auto-queries the database) or general questions (answered directly).\",\n",
    "    examples=EXAMPLE_QUERIES,\n",
    "    chatbot=gr.Chatbot(height=650),\n",
    ")\n",
    "\n",
    "app.launch(\n",
    "    server_name=\"0.0.0.0\",\n",
    "    server_port=7861,\n",
    "    share=False,\n",
    "    show_error=True,\n",
    "    theme=gr.themes.Soft(primary_hue=\"slate\", neutral_hue=\"slate\"),\n",
    "    css=CUSTOM_CSS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7861\n"
     ]
    }
   ],
   "source": [
    "app.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
